{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de conjuntos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Realizamos el cambio de directoroi de trabajo al \"Directorio Base\" que se\n",
    "current_dir = os.getcwd()\n",
    "base_path = os.path.dirname(current_dir)\n",
    "\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pasal\\Anaconda3\\envs\\PI2\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import scripts.funciones as funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y preparaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ini = dt.datetime(2017,6,1)\n",
    "d_fin = dt.datetime(2019,8,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "version = 'verFinal'\n",
    "\n",
    "### params\n",
    "freq1 = '1D'\n",
    "freq2 = '3D'\n",
    "freq3 = '7D'\n",
    "freq4 = '14D'\n",
    "freq5 = '30D'\n",
    "freq6 = '60D'\n",
    "\n",
    "### Realizamos la lectura de la informacion climatica en el rango de fechas\n",
    "### especificado, incluye la etiqueta de si ocurre o no un accidente. \n",
    "### Posteriormente, en la organizacion de la informacion climatica, lo\n",
    "### que se hace es agregar las variables con la informacion distribucional\n",
    "### de las ultimas 5 horas de la info climatica\n",
    "data = funciones.read_clima_accidentes(d_ini, d_fin, poblado = True)\n",
    "data_org = funciones.organizar_data_infoClima(data)\n",
    "\n",
    "\n",
    "### agregamos la informacion relacionada a la cantidad de accidentes ocurridas\n",
    "### en las ultimas X horas\n",
    "### Agregar senales\n",
    "senales = [freq1, freq2, freq3, freq4, freq5, freq6]\n",
    "d_ini_acc = d_ini - dt.timedelta(days = int(freq6.replace('D', '')))  ### freq mayor\n",
    "raw_accidentes = funciones.read_accidentes(d_ini_acc, d_fin)\n",
    "for fresen in senales:\n",
    "    data_org = funciones.obtener_accidentes_acumulados(data_org, \n",
    "                                                        raw_accidentes, \n",
    "                                                        freq = fresen)\n",
    "\n",
    "\n",
    "### Convertimos la bariable de Barrios en variable dummy para ser incluida\n",
    "### en el modelo\n",
    "data_org['poblado'] = data_org['BARRIO']\n",
    "data_org= pd.get_dummies(data_org, columns=['poblado'])\n",
    "\n",
    "### Relizamos la particion del conjunto de datos en las variables\n",
    "### explicativas (X) y la variable respuesta (Y)\n",
    "X = data_org.drop(columns = ['TW','BARRIO','Accidente','summary'])\n",
    "Y = data_org['Accidente'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dividimos el conjunto de datos en entrenamiento y validacion\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, \n",
    "                                                  Y,\n",
    "                                                  stratify = Y,\n",
    "                                                  test_size = 0.2,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Realizamos resampling combinando Tomek Links y Random Undersampling\n",
    "\n",
    "### Tomek Link\n",
    "tomeklinks = TomekLinks()\n",
    "X_tom, y_tom = tomeklinks.fit_sample(X_train, Y_train)\n",
    "\n",
    "### Random Undersampling\n",
    "rus = RandomUnderSampler(sampling_strategy = 30/70,random_state = 42)\n",
    "X_train_under, Y_train_under = rus.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Estandarizacion del conjunto de datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_under)\n",
    "\n",
    "X_train_under_z = pd.DataFrame(scaler.transform(X_train_under), columns = X_train_under.columns)\n",
    "X_val_z = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos eliminando conjuntos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier1 = XGBClassifier(n_estimators = 300, \n",
    "                           max_depth = 2,\n",
    "                           random_state = 42)\n",
    "\n",
    "Classifier2 = RandomForestClassifier(bootstrap=False,  \n",
    "                             criterion='entropy',\n",
    "                             max_features='auto',\n",
    "                             n_estimators=500, \n",
    "                             max_depth=10,\n",
    "                             random_state=42,\n",
    "                             warm_start=False)\n",
    "\n",
    "Classifier3 = LogisticRegression()\n",
    "\n",
    "clasificadores = [Classifier1, Classifier2, Classifier3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'vars_relevantes_final.json'\n",
    "path = os.path.join(base_path, f'models/verFinal/{file_name}')\n",
    "with open(path, 'r') as f:\n",
    "    info_vars = json.load(f)\n",
    "\n",
    "vars_voto = info_vars['voto']['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtencion_metricas(clasificadores,variables,X,Y,X_val,Y_val):\n",
    "    \n",
    "    ROC = []\n",
    "    PR = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    bAccuracY = []\n",
    "\n",
    "    for clf in clasificadores:\n",
    "        clf_mod = clone(clf)\n",
    "        clf_mod.fit(X[variables],Y)\n",
    "\n",
    "        ### Metricas en validation\n",
    "\n",
    "        preds_val = clf_mod.predict_proba(X_val[variables])\n",
    "        labels_val = clf_mod.predict(X_val[variables])\n",
    "\n",
    "        ROC_mod = metrics.roc_auc_score(Y_val,preds_val[:,1])\n",
    "        PR_mod = funciones.precision_recall_auc_score(Y_val,preds_val[:,1])\n",
    "        precision_mod = metrics.precision_score(Y_val,labels_val)\n",
    "        recall_mod = metrics.recall_score(Y_val,labels_val)\n",
    "        f1_mod = metrics.f1_score(Y_val,labels_val)\n",
    "        bAccuracY_mod = metrics.balanced_accuracy_score(Y_val,labels_val)\n",
    "\n",
    "        ROC.append(ROC_mod)\n",
    "        PR.append(PR_mod)\n",
    "        precision.append(precision_mod) \n",
    "        recall.append(recall_mod)\n",
    "        fscore.append(f1_mod)\n",
    "        bAccuracY.append(bAccuracY_mod)\n",
    "\n",
    "\n",
    "    print(f'Mean ROC: {np.mean(ROC)}')\n",
    "    print(f'Mean PR: {np.mean(PR)}')\n",
    "    print(f'Mean Precision: {np.mean(precision)}')\n",
    "    print(f'Mean Recall: {np.mean(recall)}')\n",
    "    print(f'Mean F Score: {np.mean(fscore)}')\n",
    "    print(f'Mean Balanced Accuracy: {np.mean(bAccuracY)}')\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vars elegidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.7775714076139798\n",
      "Mean PR: 0.0594891099789445\n",
      "Mean Precision: 0.0681817573505847\n",
      "Mean Recall: 0.4121037463976946\n",
      "Mean F Score: 0.11652741769906107\n",
      "Mean Balanced Accuracy: 0.6528119125609653\n"
     ]
    }
   ],
   "source": [
    "obtencion_metricas(clasificadores,vars_voto,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando variables relacionadas al barrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.7716137940052525\n",
      "Mean PR: 0.0585062674301878\n",
      "Mean Precision: 0.06674881935952924\n",
      "Mean Recall: 0.39024975984630167\n",
      "Mean F Score: 0.11386706078988806\n",
      "Mean Balanced Accuracy: 0.6439226727687714\n"
     ]
    }
   ],
   "source": [
    "vars_sinBarrio = []\n",
    "for col in vars_voto:\n",
    "    if not 'poblado' in col:\n",
    "        vars_sinBarrio.append(col)\n",
    "\n",
    "obtencion_metricas(clasificadores,vars_sinBarrio,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando variables relacionadas al barrio y senales de accidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.7302450136843225\n",
      "Mean PR: 0.04287146648881309\n",
      "Mean Precision: 0.056832906156912465\n",
      "Mean Recall: 0.1611431316042267\n",
      "Mean F Score: 0.07816517068718949\n",
      "Mean Balanced Accuracy: 0.5542402859491371\n"
     ]
    }
   ],
   "source": [
    "vars_sinBarrio_acc = []\n",
    "for col in vars_voto:\n",
    "    if (not 'poblado' in col) and (not 'cumAcc' in col):\n",
    "        vars_sinBarrio_acc.append(col)\n",
    "        \n",
    "obtencion_metricas(clasificadores,vars_sinBarrio_acc,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando variables climaticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.7717489097987461\n",
      "Mean PR: 0.05682754960893314\n",
      "Mean Precision: 0.06632320864506924\n",
      "Mean Recall: 0.3463016330451489\n",
      "Mean F Score: 0.10844431553703021\n",
      "Mean Balanced Accuracy: 0.6262128388187879\n"
     ]
    }
   ],
   "source": [
    "clima = ['precipIntensity',\n",
    "         'precipProbability',\n",
    "         'uvIndex',\n",
    "         'visibility',\n",
    "          'icon_clear-day',\n",
    "         'icon_cloudy',\n",
    "         'icon_fog',\n",
    "          'cloudCover_mean',\n",
    "         'precipIntensity_mean',\n",
    "         'visibility_mean',\n",
    "         'windSpeed_mean',\n",
    "         'cloudCover_mean_forward',\n",
    "         'dewPoint_mean_forward',\n",
    "         'precipIntensity_mean_forward',\n",
    "         'temperature_mean_forward']\n",
    "\n",
    "vars_sinClima = []\n",
    "for col in vars_voto:\n",
    "    if not col in clima:\n",
    "        vars_sinClima.append(col)\n",
    "\n",
    "obtencion_metricas(clasificadores,vars_sinClima,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando senal de accidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.7774831827017391\n",
      "Mean PR: 0.059158515615715314\n",
      "Mean Precision: 0.0719675410973866\n",
      "Mean Recall: 0.3378962536023054\n",
      "Mean F Score: 0.11491955806049829\n",
      "Mean Balanced Accuracy: 0.6267327045795296\n"
     ]
    }
   ],
   "source": [
    "vars_sinAccidente = []\n",
    "for col in vars_voto:\n",
    "    if not 'cumAcc' in col:\n",
    "        vars_sinAccidente.append(col)\n",
    "\n",
    "obtencion_metricas(clasificadores,vars_sinAccidente,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando las temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC: 0.752735956423054\n",
      "Mean PR: 0.050657436197890926\n",
      "Mean Precision: 0.05953855935532439\n",
      "Mean Recall: 0.3539865513928914\n",
      "Mean F Score: 0.10175267556421098\n",
      "Mean Balanced Accuracy: 0.6244273239285204\n"
     ]
    }
   ],
   "source": [
    "tiempo = ['hora_0', 'hora_1',\n",
    "         'hora_2', 'hora_3',\n",
    "         'hora_4', 'hora_5',\n",
    "         'hora_7', 'hora_11',\n",
    "         'hora_13', 'hora_15',\n",
    "         'hora_16', 'hora_17',\n",
    "         'hora_18', 'hora_19',\n",
    "         'hora_20', 'hora_22',\n",
    "         'hora_23',  'dia_sem_4',\n",
    "         'dia_sem_5', 'dia_sem_6',\n",
    "         'festivo', 'Mes_Abril',\n",
    "         'Mes_Agosto', 'Mes_Enero',\n",
    "         'Mes_Febrero', 'Mes_Julio',\n",
    "         'Mes_Mayo', 'Mes_Septiembre',\n",
    "         'Year_2017', 'Year_2018', 'Year_2019']\n",
    "\n",
    "vars_sinTiempo = []\n",
    "for col in vars_voto:\n",
    "    if not col in tiempo:\n",
    "        vars_sinTiempo.append(col)\n",
    "\n",
    "obtencion_metricas(clasificadores,vars_sinTiempo,X_train_under_z,Y_train_under,X_val_z,Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
